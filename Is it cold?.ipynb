{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Felt</th>\n",
       "      <th>Base Layer Top</th>\n",
       "      <th>Sweater</th>\n",
       "      <th>Base Layer Bottom</th>\n",
       "      <th>Jacket</th>\n",
       "      <th>Hood</th>\n",
       "      <th>Gloves</th>\n",
       "      <th>Scarf</th>\n",
       "      <th>Winter shoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>11:00</td>\n",
       "      <td>sunny</td>\n",
       "      <td>8</td>\n",
       "      <td>hot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>11:00</td>\n",
       "      <td>sunny</td>\n",
       "      <td>8</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>12:00</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>8</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>16:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>6</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>9:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>6</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>9:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>4</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>9:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>6</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>21:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>10</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>21:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>10</td>\n",
       "      <td>very cold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15:30</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>6</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>9:10</td>\n",
       "      <td>sun with clouds</td>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>9:38</td>\n",
       "      <td>sunny</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>11:14</td>\n",
       "      <td>sunny</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>9:56</td>\n",
       "      <td>sunny</td>\n",
       "      <td>-1</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>9:57</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>-1</td>\n",
       "      <td>cold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>18:43</td>\n",
       "      <td>rain</td>\n",
       "      <td>3</td>\n",
       "      <td>very cold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>17:00</td>\n",
       "      <td>sunny</td>\n",
       "      <td>10</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>17:00</td>\n",
       "      <td>sunny</td>\n",
       "      <td>10</td>\n",
       "      <td>cold</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   Time          Weather  Temperature       Felt  \\\n",
       "0   2018-12-24  11:00            sunny            8        hot   \n",
       "1   2018-12-24  11:00            sunny            8     normal   \n",
       "2   2018-12-25  12:00           cloudy            8       cold   \n",
       "3   2018-12-25  16:30           cloudy            6       cold   \n",
       "4   2018-12-26   9:30           cloudy            6     normal   \n",
       "5   2018-12-27   9:30           cloudy            4       cold   \n",
       "6   2018-12-28   9:30           cloudy            6     normal   \n",
       "7   2018-12-29  21:30           cloudy           10       cold   \n",
       "8   2018-12-29  21:30           cloudy           10  very cold   \n",
       "9   2019-01-02  15:30           cloudy            6       cold   \n",
       "10  2019-01-14   9:10  sun with clouds            5     normal   \n",
       "11  2019-01-18   9:38            sunny            1     normal   \n",
       "12  2019-01-19  11:14            sunny            1     normal   \n",
       "13  2019-01-21   9:56            sunny           -1       cold   \n",
       "14  2019-01-22   9:57           cloudy           -1       cold   \n",
       "15  2019-02-04  18:43             rain            3  very cold   \n",
       "16  2019-02-24  17:00            sunny           10     normal   \n",
       "17  2019-02-24  17:00            sunny           10       cold   \n",
       "\n",
       "    Base Layer Top  Sweater  Base Layer Bottom  Jacket  Hood  Gloves  Scarf  \\\n",
       "0                1        0                  1       1     1       1      1   \n",
       "1                1        0                  0       1     1       0      0   \n",
       "2                1        1                  1       1     1       0      0   \n",
       "3                1        1                  1       1     1       0      0   \n",
       "4                1        0                  1       1     1       1      1   \n",
       "5                1        0                  1       1     1       1      1   \n",
       "6                1        0                  0       1     1       1      1   \n",
       "7                1        0                  1       1     1       1      0   \n",
       "8                1        0                  0       1     1       1      0   \n",
       "9                1        0                  1       1     1       1      0   \n",
       "10               1        1                  1       1     1       1      0   \n",
       "11               1        1                  1       1     1       1      0   \n",
       "12               1        1                  1       1     1       1      0   \n",
       "13               1        1                  1       1     1       1      0   \n",
       "14               1        1                  1       2     1       1      1   \n",
       "15               1        1                  1       2     1       2      1   \n",
       "16               1        0                  1       1     1       0      0   \n",
       "17               0        0                  0       1     0       0      0   \n",
       "\n",
       "    Winter shoes  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              1  \n",
       "10             1  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             1  \n",
       "15             1  \n",
       "16             0  \n",
       "17             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/is_it_cold.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Felt</th>\n",
       "      <th>Base Layer Top</th>\n",
       "      <th>Sweater</th>\n",
       "      <th>Base Layer Bottom</th>\n",
       "      <th>Jacket</th>\n",
       "      <th>Hood</th>\n",
       "      <th>Gloves</th>\n",
       "      <th>Scarf</th>\n",
       "      <th>Winter shoes</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloudy</th>\n",
       "      <th>sun with clouds</th>\n",
       "      <th>sunny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>0.037838</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>0.167568</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>0.062162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>0.063514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>0.774324</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time  Temperature  Felt  Base Layer Top  Sweater  \\\n",
       "0   2018-12-24  0.148649     0.818182  1.00               1        0   \n",
       "1   2018-12-24  0.148649     0.818182  0.75               1        0   \n",
       "2   2018-12-25  0.229730     0.818182  0.50               1        1   \n",
       "3   2018-12-25  0.594595     0.636364  0.50               1        1   \n",
       "4   2018-12-26  0.027027     0.636364  0.75               1        0   \n",
       "5   2018-12-27  0.027027     0.454545  0.50               1        0   \n",
       "6   2018-12-28  0.027027     0.636364  0.75               1        0   \n",
       "7   2018-12-29  1.000000     1.000000  0.50               1        0   \n",
       "8   2018-12-29  1.000000     1.000000  0.00               1        0   \n",
       "9   2019-01-02  0.513514     0.636364  0.50               1        0   \n",
       "10  2019-01-14  0.000000     0.545455  0.75               1        1   \n",
       "11  2019-01-18  0.037838     0.181818  0.75               1        1   \n",
       "12  2019-01-19  0.167568     0.181818  0.75               1        1   \n",
       "13  2019-01-21  0.062162     0.000000  0.50               1        1   \n",
       "14  2019-01-22  0.063514     0.000000  0.50               1        1   \n",
       "15  2019-02-04  0.774324     0.363636  0.00               1        1   \n",
       "16  2019-02-24  0.635135     1.000000  0.75               1        0   \n",
       "17  2019-02-24  0.635135     1.000000  0.50               0        0   \n",
       "\n",
       "    Base Layer Bottom  Jacket  Hood  Gloves  Scarf  Winter shoes  rain  \\\n",
       "0                   1       1     1       1      1             1     0   \n",
       "1                   0       1     1       0      0             1     0   \n",
       "2                   1       1     1       0      0             0     0   \n",
       "3                   1       1     1       0      0             0     0   \n",
       "4                   1       1     1       1      1             1     0   \n",
       "5                   1       1     1       1      1             1     0   \n",
       "6                   0       1     1       1      1             1     0   \n",
       "7                   1       1     1       1      0             1     0   \n",
       "8                   0       1     1       1      0             1     0   \n",
       "9                   1       1     1       1      0             1     0   \n",
       "10                  1       1     1       1      0             1     0   \n",
       "11                  1       1     1       1      0             1     0   \n",
       "12                  1       1     1       1      0             1     0   \n",
       "13                  1       1     1       1      0             1     0   \n",
       "14                  1       2     1       1      1             1     0   \n",
       "15                  1       2     1       2      1             1     1   \n",
       "16                  1       1     1       0      0             0     0   \n",
       "17                  0       1     0       0      0             0     0   \n",
       "\n",
       "    cloudy  sun with clouds  sunny  \n",
       "0        0                0      1  \n",
       "1        0                0      1  \n",
       "2        1                0      0  \n",
       "3        1                0      0  \n",
       "4        1                0      0  \n",
       "5        1                0      0  \n",
       "6        1                0      0  \n",
       "7        1                0      0  \n",
       "8        1                0      0  \n",
       "9        1                0      0  \n",
       "10       0                1      0  \n",
       "11       0                0      1  \n",
       "12       0                0      1  \n",
       "13       0                0      1  \n",
       "14       1                0      0  \n",
       "15       0                0      0  \n",
       "16       0                0      1  \n",
       "17       0                0      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df['Felt'] = [{'very cold': -3, 'cold': -1, 'normal': 0, 'hot': 1, 'very hot': 3}[x] for x in df['Felt']]\n",
    "\n",
    "    weather = pd.Categorical(df['Weather'], categories=['rain', 'cloudy', 'sun with clouds', 'sunny'])\n",
    "    df = pd.concat([df, pd.get_dummies(weather)], axis=1).drop(columns=['Weather'])\n",
    "\n",
    "    time = pd.DatetimeIndex(df['Time'])\n",
    "    df['Time'] = time.hour * 60 + time.minute\n",
    "    \n",
    "    scaled = MinMaxScaler().fit_transform(df[['Time', 'Temperature', 'Felt']])\n",
    "    df['Time'] = scaled[:,0]\n",
    "    df['Temperature'] = scaled[:,1]\n",
    "    df['Felt'] = scaled[:,2]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027554933449074073"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clothes = ['Sweater', 'Base Layer Top', 'Base Layer Bottom', 'Jacket', 'Hood', 'Gloves', 'Scarf', 'Winter shoes']\n",
    "X = df.drop(columns=clothes).drop(columns=['Date'])\n",
    "Y = df[clothes]\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "model = MultiOutputRegressor(forest, n_jobs=-1)\n",
    "model.fit(X, Y)#.score(X, Y)\n",
    "mean_squared_error(model.predict(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 408       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 72        \n",
      "=================================================================\n",
      "Total params: 994\n",
      "Trainable params: 994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18 samples, validate on 18 samples\n",
      "Epoch 1/1000\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.7077 - mean_squared_error: 0.6364 - val_loss: 0.7069 - val_mean_squared_error: 0.6351\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 0s 232us/step - loss: 0.7065 - mean_squared_error: 0.6354 - val_loss: 0.7066 - val_mean_squared_error: 0.6347\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 0s 243us/step - loss: 0.7094 - mean_squared_error: 0.6379 - val_loss: 0.7062 - val_mean_squared_error: 0.6342\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 0s 238us/step - loss: 0.7072 - mean_squared_error: 0.6355 - val_loss: 0.7059 - val_mean_squared_error: 0.6338\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 0s 218us/step - loss: 0.7052 - mean_squared_error: 0.6333 - val_loss: 0.7055 - val_mean_squared_error: 0.6333\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 0s 203us/step - loss: 0.7050 - mean_squared_error: 0.6328 - val_loss: 0.7051 - val_mean_squared_error: 0.6329\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 0s 195us/step - loss: 0.7039 - mean_squared_error: 0.6328 - val_loss: 0.7047 - val_mean_squared_error: 0.6325\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 0s 283us/step - loss: 0.7036 - mean_squared_error: 0.6316 - val_loss: 0.7044 - val_mean_squared_error: 0.6320\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 0s 800us/step - loss: 0.7043 - mean_squared_error: 0.6319 - val_loss: 0.7040 - val_mean_squared_error: 0.6316\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 0s 263us/step - loss: 0.7047 - mean_squared_error: 0.6330 - val_loss: 0.7036 - val_mean_squared_error: 0.6312\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7045 - mean_squared_error: 0.6324 - val_loss: 0.7032 - val_mean_squared_error: 0.6307\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 0s 455us/step - loss: 0.7008 - mean_squared_error: 0.6287 - val_loss: 0.7029 - val_mean_squared_error: 0.6303\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 0s 451us/step - loss: 0.7057 - mean_squared_error: 0.6334 - val_loss: 0.7025 - val_mean_squared_error: 0.6299\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 0s 274us/step - loss: 0.7057 - mean_squared_error: 0.6342 - val_loss: 0.7021 - val_mean_squared_error: 0.6294\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 0s 239us/step - loss: 0.7001 - mean_squared_error: 0.6276 - val_loss: 0.7017 - val_mean_squared_error: 0.6290\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 0s 188us/step - loss: 0.7022 - mean_squared_error: 0.6297 - val_loss: 0.7013 - val_mean_squared_error: 0.6286\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 0s 403us/step - loss: 0.7018 - mean_squared_error: 0.6290 - val_loss: 0.7009 - val_mean_squared_error: 0.6281\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 0s 760us/step - loss: 0.7004 - mean_squared_error: 0.6289 - val_loss: 0.7005 - val_mean_squared_error: 0.6277\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 0s 262us/step - loss: 0.7001 - mean_squared_error: 0.6271 - val_loss: 0.7001 - val_mean_squared_error: 0.6273\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 0s 922us/step - loss: 0.7026 - mean_squared_error: 0.6293 - val_loss: 0.6997 - val_mean_squared_error: 0.6268\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 0s 618us/step - loss: 0.6982 - mean_squared_error: 0.6263 - val_loss: 0.6993 - val_mean_squared_error: 0.6264\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 0s 615us/step - loss: 0.7012 - mean_squared_error: 0.6292 - val_loss: 0.6989 - val_mean_squared_error: 0.6260\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 0s 670us/step - loss: 0.6978 - mean_squared_error: 0.6259 - val_loss: 0.6985 - val_mean_squared_error: 0.6255\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 0s 554us/step - loss: 0.7001 - mean_squared_error: 0.6277 - val_loss: 0.6981 - val_mean_squared_error: 0.6251\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 0s 803us/step - loss: 0.6987 - mean_squared_error: 0.6265 - val_loss: 0.6977 - val_mean_squared_error: 0.6247\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 0s 513us/step - loss: 0.6991 - mean_squared_error: 0.6270 - val_loss: 0.6972 - val_mean_squared_error: 0.6243\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 0s 562us/step - loss: 0.6976 - mean_squared_error: 0.6252 - val_loss: 0.6968 - val_mean_squared_error: 0.6238\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 0s 421us/step - loss: 0.6971 - mean_squared_error: 0.6247 - val_loss: 0.6964 - val_mean_squared_error: 0.6234\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 0s 440us/step - loss: 0.6951 - mean_squared_error: 0.6224 - val_loss: 0.6959 - val_mean_squared_error: 0.6230\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 0s 530us/step - loss: 0.6959 - mean_squared_error: 0.6233 - val_loss: 0.6955 - val_mean_squared_error: 0.6225\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 0s 357us/step - loss: 0.6955 - mean_squared_error: 0.6234 - val_loss: 0.6951 - val_mean_squared_error: 0.6221\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 0s 316us/step - loss: 0.6935 - mean_squared_error: 0.6211 - val_loss: 0.6946 - val_mean_squared_error: 0.6217\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 0s 260us/step - loss: 0.6931 - mean_squared_error: 0.6215 - val_loss: 0.6941 - val_mean_squared_error: 0.6213\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 0s 366us/step - loss: 0.6935 - mean_squared_error: 0.6207 - val_loss: 0.6936 - val_mean_squared_error: 0.6208\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 0s 220us/step - loss: 0.6948 - mean_squared_error: 0.6230 - val_loss: 0.6931 - val_mean_squared_error: 0.6204\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6923 - mean_squared_error: 0.6211 - val_loss: 0.6926 - val_mean_squared_error: 0.6199\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 0s 491us/step - loss: 0.6907 - mean_squared_error: 0.6181 - val_loss: 0.6921 - val_mean_squared_error: 0.6194\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 0s 338us/step - loss: 0.6928 - mean_squared_error: 0.6213 - val_loss: 0.6915 - val_mean_squared_error: 0.6190\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 0s 265us/step - loss: 0.6928 - mean_squared_error: 0.6201 - val_loss: 0.6910 - val_mean_squared_error: 0.6185\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 0s 601us/step - loss: 0.6889 - mean_squared_error: 0.6161 - val_loss: 0.6904 - val_mean_squared_error: 0.6180\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 0s 345us/step - loss: 0.6888 - mean_squared_error: 0.6161 - val_loss: 0.6898 - val_mean_squared_error: 0.6175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000\n",
      "18/18 [==============================] - 0s 247us/step - loss: 0.6890 - mean_squared_error: 0.6172 - val_loss: 0.6892 - val_mean_squared_error: 0.6170\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 0s 271us/step - loss: 0.6894 - mean_squared_error: 0.6177 - val_loss: 0.6886 - val_mean_squared_error: 0.6165\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 0s 206us/step - loss: 0.6890 - mean_squared_error: 0.6171 - val_loss: 0.6880 - val_mean_squared_error: 0.6161\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 0s 296us/step - loss: 0.6886 - mean_squared_error: 0.6171 - val_loss: 0.6873 - val_mean_squared_error: 0.6156\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 0s 499us/step - loss: 0.6874 - mean_squared_error: 0.6164 - val_loss: 0.6867 - val_mean_squared_error: 0.6151\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 0s 269us/step - loss: 0.6905 - mean_squared_error: 0.6187 - val_loss: 0.6860 - val_mean_squared_error: 0.6146\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 0s 271us/step - loss: 0.6898 - mean_squared_error: 0.6183 - val_loss: 0.6854 - val_mean_squared_error: 0.6141\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 0s 616us/step - loss: 0.6838 - mean_squared_error: 0.6142 - val_loss: 0.6847 - val_mean_squared_error: 0.6137\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 0s 293us/step - loss: 0.6865 - mean_squared_error: 0.6144 - val_loss: 0.6840 - val_mean_squared_error: 0.6132\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6846 - mean_squared_error: 0.6135 - val_loss: 0.6833 - val_mean_squared_error: 0.6127\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 0s 301us/step - loss: 0.6853 - mean_squared_error: 0.6154 - val_loss: 0.6826 - val_mean_squared_error: 0.6122\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 0s 466us/step - loss: 0.6851 - mean_squared_error: 0.6150 - val_loss: 0.6819 - val_mean_squared_error: 0.6118\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 0s 276us/step - loss: 0.6831 - mean_squared_error: 0.6128 - val_loss: 0.6811 - val_mean_squared_error: 0.6113\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 0s 382us/step - loss: 0.6800 - mean_squared_error: 0.6112 - val_loss: 0.6804 - val_mean_squared_error: 0.6109\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 0s 227us/step - loss: 0.6802 - mean_squared_error: 0.6115 - val_loss: 0.6796 - val_mean_squared_error: 0.6104\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 0s 320us/step - loss: 0.6788 - mean_squared_error: 0.6107 - val_loss: 0.6788 - val_mean_squared_error: 0.6100\n",
      "Epoch 58/1000\n",
      "18/18 [==============================] - 0s 630us/step - loss: 0.6795 - mean_squared_error: 0.6106 - val_loss: 0.6781 - val_mean_squared_error: 0.6095\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 0s 283us/step - loss: 0.6825 - mean_squared_error: 0.6132 - val_loss: 0.6773 - val_mean_squared_error: 0.6091\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 0s 322us/step - loss: 0.6829 - mean_squared_error: 0.6138 - val_loss: 0.6765 - val_mean_squared_error: 0.6087\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 0s 321us/step - loss: 0.6793 - mean_squared_error: 0.6098 - val_loss: 0.6757 - val_mean_squared_error: 0.6083\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 0s 670us/step - loss: 0.6765 - mean_squared_error: 0.6090 - val_loss: 0.6749 - val_mean_squared_error: 0.6079\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 0s 441us/step - loss: 0.6764 - mean_squared_error: 0.6093 - val_loss: 0.6741 - val_mean_squared_error: 0.6076\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 0s 511us/step - loss: 0.6739 - mean_squared_error: 0.6097 - val_loss: 0.6733 - val_mean_squared_error: 0.6072\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 0s 385us/step - loss: 0.6772 - mean_squared_error: 0.6095 - val_loss: 0.6725 - val_mean_squared_error: 0.6069\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 0s 711us/step - loss: 0.6751 - mean_squared_error: 0.6089 - val_loss: 0.6717 - val_mean_squared_error: 0.6065\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 0s 543us/step - loss: 0.6725 - mean_squared_error: 0.6067 - val_loss: 0.6709 - val_mean_squared_error: 0.6062\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 0s 925us/step - loss: 0.6716 - mean_squared_error: 0.6073 - val_loss: 0.6701 - val_mean_squared_error: 0.6060\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 0s 660us/step - loss: 0.6742 - mean_squared_error: 0.6073 - val_loss: 0.6693 - val_mean_squared_error: 0.6057\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 0s 636us/step - loss: 0.6739 - mean_squared_error: 0.6093 - val_loss: 0.6685 - val_mean_squared_error: 0.6054\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 0s 532us/step - loss: 0.6744 - mean_squared_error: 0.6089 - val_loss: 0.6678 - val_mean_squared_error: 0.6052\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 0s 613us/step - loss: 0.6733 - mean_squared_error: 0.6097 - val_loss: 0.6670 - val_mean_squared_error: 0.6050\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 0s 513us/step - loss: 0.6689 - mean_squared_error: 0.6063 - val_loss: 0.6663 - val_mean_squared_error: 0.6048\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 0s 434us/step - loss: 0.6692 - mean_squared_error: 0.6064 - val_loss: 0.6656 - val_mean_squared_error: 0.6046\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 0s 423us/step - loss: 0.6687 - mean_squared_error: 0.6058 - val_loss: 0.6649 - val_mean_squared_error: 0.6045\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 0s 518us/step - loss: 0.6687 - mean_squared_error: 0.6075 - val_loss: 0.6642 - val_mean_squared_error: 0.6043\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 0s 382us/step - loss: 0.6648 - mean_squared_error: 0.6055 - val_loss: 0.6635 - val_mean_squared_error: 0.6042\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 0s 358us/step - loss: 0.6670 - mean_squared_error: 0.6041 - val_loss: 0.6628 - val_mean_squared_error: 0.6041\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 0s 587us/step - loss: 0.6627 - mean_squared_error: 0.6024 - val_loss: 0.6622 - val_mean_squared_error: 0.6040\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 0s 320us/step - loss: 0.6684 - mean_squared_error: 0.6077 - val_loss: 0.6615 - val_mean_squared_error: 0.6040\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 0s 265us/step - loss: 0.6655 - mean_squared_error: 0.6050 - val_loss: 0.6609 - val_mean_squared_error: 0.6039\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 0s 466us/step - loss: 0.6641 - mean_squared_error: 0.6025 - val_loss: 0.6603 - val_mean_squared_error: 0.6039\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 0s 323us/step - loss: 0.6637 - mean_squared_error: 0.6065 - val_loss: 0.6598 - val_mean_squared_error: 0.6038\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 0s 185us/step - loss: 0.6601 - mean_squared_error: 0.6035 - val_loss: 0.6592 - val_mean_squared_error: 0.6038\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 0s 422us/step - loss: 0.6603 - mean_squared_error: 0.6029 - val_loss: 0.6587 - val_mean_squared_error: 0.6038\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 0s 401us/step - loss: 0.6605 - mean_squared_error: 0.6035 - val_loss: 0.6581 - val_mean_squared_error: 0.6038\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 0s 341us/step - loss: 0.6606 - mean_squared_error: 0.6035 - val_loss: 0.6577 - val_mean_squared_error: 0.6039\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 0s 314us/step - loss: 0.6604 - mean_squared_error: 0.6056 - val_loss: 0.6572 - val_mean_squared_error: 0.6039\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 0s 907us/step - loss: 0.6588 - mean_squared_error: 0.6038 - val_loss: 0.6567 - val_mean_squared_error: 0.6040\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 0s 399us/step - loss: 0.6571 - mean_squared_error: 0.6048 - val_loss: 0.6563 - val_mean_squared_error: 0.6040\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6606 - mean_squared_error: 0.6044 - val_loss: 0.6558 - val_mean_squared_error: 0.6041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/1000\n",
      "18/18 [==============================] - 0s 286us/step - loss: 0.6591 - mean_squared_error: 0.6037 - val_loss: 0.6554 - val_mean_squared_error: 0.6041\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 0s 467us/step - loss: 0.6580 - mean_squared_error: 0.6038 - val_loss: 0.6550 - val_mean_squared_error: 0.6042\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 0s 336us/step - loss: 0.6577 - mean_squared_error: 0.6034 - val_loss: 0.6546 - val_mean_squared_error: 0.6043\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 0s 590us/step - loss: 0.6583 - mean_squared_error: 0.6051 - val_loss: 0.6543 - val_mean_squared_error: 0.6044\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 0s 380us/step - loss: 0.6575 - mean_squared_error: 0.6039 - val_loss: 0.6539 - val_mean_squared_error: 0.6045\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 0s 311us/step - loss: 0.6567 - mean_squared_error: 0.6029 - val_loss: 0.6536 - val_mean_squared_error: 0.6046\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 0s 586us/step - loss: 0.6551 - mean_squared_error: 0.6066 - val_loss: 0.6533 - val_mean_squared_error: 0.6047\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 0s 315us/step - loss: 0.6536 - mean_squared_error: 0.6038 - val_loss: 0.6529 - val_mean_squared_error: 0.6048\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 0s 353us/step - loss: 0.6561 - mean_squared_error: 0.6074 - val_loss: 0.6526 - val_mean_squared_error: 0.6050\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 0s 904us/step - loss: 0.6549 - mean_squared_error: 0.6037 - val_loss: 0.6524 - val_mean_squared_error: 0.6051\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 0s 388us/step - loss: 0.6586 - mean_squared_error: 0.6058 - val_loss: 0.6521 - val_mean_squared_error: 0.6052\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 0s 364us/step - loss: 0.6556 - mean_squared_error: 0.6052 - val_loss: 0.6518 - val_mean_squared_error: 0.6054\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 0s 359us/step - loss: 0.6549 - mean_squared_error: 0.6071 - val_loss: 0.6515 - val_mean_squared_error: 0.6055\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 0s 368us/step - loss: 0.6521 - mean_squared_error: 0.6040 - val_loss: 0.6513 - val_mean_squared_error: 0.6056\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 0s 338us/step - loss: 0.6539 - mean_squared_error: 0.6037 - val_loss: 0.6510 - val_mean_squared_error: 0.6058\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 0s 619us/step - loss: 0.6526 - mean_squared_error: 0.6046 - val_loss: 0.6508 - val_mean_squared_error: 0.6060\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 0s 510us/step - loss: 0.6526 - mean_squared_error: 0.6068 - val_loss: 0.6506 - val_mean_squared_error: 0.6061\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 0s 338us/step - loss: 0.6515 - mean_squared_error: 0.6048 - val_loss: 0.6503 - val_mean_squared_error: 0.6063\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 0s 393us/step - loss: 0.6530 - mean_squared_error: 0.6071 - val_loss: 0.6501 - val_mean_squared_error: 0.6065\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 0s 345us/step - loss: 0.6528 - mean_squared_error: 0.6080 - val_loss: 0.6499 - val_mean_squared_error: 0.6066\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 0s 564us/step - loss: 0.6514 - mean_squared_error: 0.6064 - val_loss: 0.6497 - val_mean_squared_error: 0.6068\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 0s 439us/step - loss: 0.6521 - mean_squared_error: 0.6068 - val_loss: 0.6495 - val_mean_squared_error: 0.6070\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 0s 311us/step - loss: 0.6507 - mean_squared_error: 0.6068 - val_loss: 0.6493 - val_mean_squared_error: 0.6072\n",
      "Epoch 115/1000\n",
      "18/18 [==============================] - 0s 446us/step - loss: 0.6497 - mean_squared_error: 0.6061 - val_loss: 0.6491 - val_mean_squared_error: 0.6074\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 0s 379us/step - loss: 0.6504 - mean_squared_error: 0.6062 - val_loss: 0.6490 - val_mean_squared_error: 0.6076\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 0s 390us/step - loss: 0.6504 - mean_squared_error: 0.6065 - val_loss: 0.6488 - val_mean_squared_error: 0.6078\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 0s 399us/step - loss: 0.6495 - mean_squared_error: 0.6057 - val_loss: 0.6486 - val_mean_squared_error: 0.6080\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 0s 508us/step - loss: 0.6501 - mean_squared_error: 0.6081 - val_loss: 0.6485 - val_mean_squared_error: 0.6082\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 0s 342us/step - loss: 0.6490 - mean_squared_error: 0.6084 - val_loss: 0.6483 - val_mean_squared_error: 0.6084\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 0s 442us/step - loss: 0.6506 - mean_squared_error: 0.6079 - val_loss: 0.6482 - val_mean_squared_error: 0.6086\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 0s 401us/step - loss: 0.6495 - mean_squared_error: 0.6088 - val_loss: 0.6480 - val_mean_squared_error: 0.6089\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 0s 352us/step - loss: 0.6497 - mean_squared_error: 0.6076 - val_loss: 0.6479 - val_mean_squared_error: 0.6091\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 0s 296us/step - loss: 0.6485 - mean_squared_error: 0.6085 - val_loss: 0.6477 - val_mean_squared_error: 0.6093\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 0s 308us/step - loss: 0.6488 - mean_squared_error: 0.6090 - val_loss: 0.6476 - val_mean_squared_error: 0.6096\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 0s 238us/step - loss: 0.6509 - mean_squared_error: 0.6081 - val_loss: 0.6474 - val_mean_squared_error: 0.6098\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 0s 342us/step - loss: 0.6497 - mean_squared_error: 0.6083 - val_loss: 0.6473 - val_mean_squared_error: 0.6101\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 0s 402us/step - loss: 0.6492 - mean_squared_error: 0.6095 - val_loss: 0.6472 - val_mean_squared_error: 0.6103\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 0s 280us/step - loss: 0.6480 - mean_squared_error: 0.6104 - val_loss: 0.6471 - val_mean_squared_error: 0.6106\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6471 - mean_squared_error: 0.6097 - val_loss: 0.6469 - val_mean_squared_error: 0.6109\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 0s 264us/step - loss: 0.6485 - mean_squared_error: 0.6113 - val_loss: 0.6468 - val_mean_squared_error: 0.6111\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 0s 315us/step - loss: 0.6473 - mean_squared_error: 0.6099 - val_loss: 0.6467 - val_mean_squared_error: 0.6114\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 0s 575us/step - loss: 0.6474 - mean_squared_error: 0.6121 - val_loss: 0.6466 - val_mean_squared_error: 0.6117\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 0s 453us/step - loss: 0.6479 - mean_squared_error: 0.6102 - val_loss: 0.6465 - val_mean_squared_error: 0.6120\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 0s 187us/step - loss: 0.6476 - mean_squared_error: 0.6105 - val_loss: 0.6464 - val_mean_squared_error: 0.6123\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 0s 350us/step - loss: 0.6481 - mean_squared_error: 0.6107 - val_loss: 0.6462 - val_mean_squared_error: 0.6126\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 0s 329us/step - loss: 0.6469 - mean_squared_error: 0.6116 - val_loss: 0.6461 - val_mean_squared_error: 0.6128\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 0s 293us/step - loss: 0.6476 - mean_squared_error: 0.6109 - val_loss: 0.6460 - val_mean_squared_error: 0.6131\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 0s 213us/step - loss: 0.6467 - mean_squared_error: 0.6125 - val_loss: 0.6459 - val_mean_squared_error: 0.6134\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 0s 428us/step - loss: 0.6460 - mean_squared_error: 0.6119 - val_loss: 0.6458 - val_mean_squared_error: 0.6137\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 0s 437us/step - loss: 0.6460 - mean_squared_error: 0.6107 - val_loss: 0.6457 - val_mean_squared_error: 0.6140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1000\n",
      "18/18 [==============================] - 0s 249us/step - loss: 0.6472 - mean_squared_error: 0.6124 - val_loss: 0.6456 - val_mean_squared_error: 0.6143\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 0s 423us/step - loss: 0.6459 - mean_squared_error: 0.6129 - val_loss: 0.6455 - val_mean_squared_error: 0.6146\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 0s 357us/step - loss: 0.6458 - mean_squared_error: 0.6144 - val_loss: 0.6455 - val_mean_squared_error: 0.6149\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 0s 391us/step - loss: 0.6462 - mean_squared_error: 0.6126 - val_loss: 0.6454 - val_mean_squared_error: 0.6152\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 0s 487us/step - loss: 0.6459 - mean_squared_error: 0.6132 - val_loss: 0.6453 - val_mean_squared_error: 0.6155\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6459 - mean_squared_error: 0.6141 - val_loss: 0.6452 - val_mean_squared_error: 0.6159\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 0s 275us/step - loss: 0.6462 - mean_squared_error: 0.6126 - val_loss: 0.6451 - val_mean_squared_error: 0.6162\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 0s 419us/step - loss: 0.6466 - mean_squared_error: 0.6123 - val_loss: 0.6450 - val_mean_squared_error: 0.6165\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 0s 421us/step - loss: 0.6451 - mean_squared_error: 0.6149 - val_loss: 0.6449 - val_mean_squared_error: 0.6168\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 0s 292us/step - loss: 0.6464 - mean_squared_error: 0.6141 - val_loss: 0.6449 - val_mean_squared_error: 0.6171\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 0s 541us/step - loss: 0.6475 - mean_squared_error: 0.6148 - val_loss: 0.6448 - val_mean_squared_error: 0.6174\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 0s 355us/step - loss: 0.6451 - mean_squared_error: 0.6169 - val_loss: 0.6447 - val_mean_squared_error: 0.6177\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 0s 298us/step - loss: 0.6449 - mean_squared_error: 0.6167 - val_loss: 0.6446 - val_mean_squared_error: 0.6180\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 0s 282us/step - loss: 0.6470 - mean_squared_error: 0.6169 - val_loss: 0.6445 - val_mean_squared_error: 0.6183\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 0s 369us/step - loss: 0.6448 - mean_squared_error: 0.6168 - val_loss: 0.6445 - val_mean_squared_error: 0.6186\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 0s 429us/step - loss: 0.6451 - mean_squared_error: 0.6153 - val_loss: 0.6444 - val_mean_squared_error: 0.6189\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 0s 286us/step - loss: 0.6458 - mean_squared_error: 0.6161 - val_loss: 0.6443 - val_mean_squared_error: 0.6192\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 0s 561us/step - loss: 0.6462 - mean_squared_error: 0.6184 - val_loss: 0.6443 - val_mean_squared_error: 0.6195\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 0s 246us/step - loss: 0.6444 - mean_squared_error: 0.6190 - val_loss: 0.6442 - val_mean_squared_error: 0.6198\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 0s 311us/step - loss: 0.6448 - mean_squared_error: 0.6194 - val_loss: 0.6441 - val_mean_squared_error: 0.6201\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 0s 334us/step - loss: 0.6444 - mean_squared_error: 0.6199 - val_loss: 0.6441 - val_mean_squared_error: 0.6204\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 0s 449us/step - loss: 0.6454 - mean_squared_error: 0.6188 - val_loss: 0.6440 - val_mean_squared_error: 0.6207\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 0s 276us/step - loss: 0.6450 - mean_squared_error: 0.6186 - val_loss: 0.6439 - val_mean_squared_error: 0.6210\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 0s 275us/step - loss: 0.6442 - mean_squared_error: 0.6197 - val_loss: 0.6439 - val_mean_squared_error: 0.6213\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 0s 422us/step - loss: 0.6446 - mean_squared_error: 0.6208 - val_loss: 0.6438 - val_mean_squared_error: 0.6216\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 0s 581us/step - loss: 0.6441 - mean_squared_error: 0.6201 - val_loss: 0.6437 - val_mean_squared_error: 0.6219\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 0s 504us/step - loss: 0.6440 - mean_squared_error: 0.6207 - val_loss: 0.6437 - val_mean_squared_error: 0.6222\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 0s 481us/step - loss: 0.6440 - mean_squared_error: 0.6203 - val_loss: 0.6436 - val_mean_squared_error: 0.6225\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 0s 414us/step - loss: 0.6441 - mean_squared_error: 0.6193 - val_loss: 0.6436 - val_mean_squared_error: 0.6228\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 0s 440us/step - loss: 0.6438 - mean_squared_error: 0.6210 - val_loss: 0.6435 - val_mean_squared_error: 0.6231\n",
      "Epoch 172/1000\n",
      "18/18 [==============================] - 0s 248us/step - loss: 0.6439 - mean_squared_error: 0.6220 - val_loss: 0.6434 - val_mean_squared_error: 0.6234\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 0s 437us/step - loss: 0.6444 - mean_squared_error: 0.6230 - val_loss: 0.6434 - val_mean_squared_error: 0.6236\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 0s 449us/step - loss: 0.6441 - mean_squared_error: 0.6228 - val_loss: 0.6433 - val_mean_squared_error: 0.6239\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 0s 457us/step - loss: 0.6437 - mean_squared_error: 0.6227 - val_loss: 0.6433 - val_mean_squared_error: 0.6242\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 0s 461us/step - loss: 0.6440 - mean_squared_error: 0.6223 - val_loss: 0.6432 - val_mean_squared_error: 0.6245\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 0s 454us/step - loss: 0.6451 - mean_squared_error: 0.6238 - val_loss: 0.6432 - val_mean_squared_error: 0.6248\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 0s 516us/step - loss: 0.6435 - mean_squared_error: 0.6241 - val_loss: 0.6431 - val_mean_squared_error: 0.6250\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 0s 487us/step - loss: 0.6435 - mean_squared_error: 0.6245 - val_loss: 0.6431 - val_mean_squared_error: 0.6253\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 0s 319us/step - loss: 0.6437 - mean_squared_error: 0.6239 - val_loss: 0.6430 - val_mean_squared_error: 0.6256\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.6437 - mean_squared_error: 0.6253 - val_loss: 0.6430 - val_mean_squared_error: 0.6259\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 0s 631us/step - loss: 0.6432 - mean_squared_error: 0.6236 - val_loss: 0.6429 - val_mean_squared_error: 0.6261\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 0s 411us/step - loss: 0.6433 - mean_squared_error: 0.6240 - val_loss: 0.6429 - val_mean_squared_error: 0.6264\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 0s 331us/step - loss: 0.6439 - mean_squared_error: 0.6223 - val_loss: 0.6428 - val_mean_squared_error: 0.6266\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 0s 201us/step - loss: 0.6435 - mean_squared_error: 0.6240 - val_loss: 0.6428 - val_mean_squared_error: 0.6269\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 0s 304us/step - loss: 0.6434 - mean_squared_error: 0.6244 - val_loss: 0.6427 - val_mean_squared_error: 0.6272\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 0s 648us/step - loss: 0.6433 - mean_squared_error: 0.6260 - val_loss: 0.6427 - val_mean_squared_error: 0.6274\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 0s 205us/step - loss: 0.6430 - mean_squared_error: 0.6262 - val_loss: 0.6427 - val_mean_squared_error: 0.6277\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 0s 337us/step - loss: 0.6439 - mean_squared_error: 0.6272 - val_loss: 0.6426 - val_mean_squared_error: 0.6279\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 0s 301us/step - loss: 0.6427 - mean_squared_error: 0.6268 - val_loss: 0.6426 - val_mean_squared_error: 0.6282\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 0s 294us/step - loss: 0.6429 - mean_squared_error: 0.6265 - val_loss: 0.6425 - val_mean_squared_error: 0.6284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/1000\n",
      "18/18 [==============================] - 0s 275us/step - loss: 0.6429 - mean_squared_error: 0.6269 - val_loss: 0.6425 - val_mean_squared_error: 0.6286\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 0s 415us/step - loss: 0.6431 - mean_squared_error: 0.6279 - val_loss: 0.6424 - val_mean_squared_error: 0.6289\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 0s 293us/step - loss: 0.6433 - mean_squared_error: 0.6278 - val_loss: 0.6424 - val_mean_squared_error: 0.6291\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 0s 264us/step - loss: 0.6426 - mean_squared_error: 0.6298 - val_loss: 0.6424 - val_mean_squared_error: 0.6293\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 0s 754us/step - loss: 0.6424 - mean_squared_error: 0.6284 - val_loss: 0.6423 - val_mean_squared_error: 0.6296\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 0s 680us/step - loss: 0.6429 - mean_squared_error: 0.6269 - val_loss: 0.6423 - val_mean_squared_error: 0.6298\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 0s 600us/step - loss: 0.6427 - mean_squared_error: 0.6269 - val_loss: 0.6423 - val_mean_squared_error: 0.6300\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 0s 596us/step - loss: 0.6436 - mean_squared_error: 0.6287 - val_loss: 0.6422 - val_mean_squared_error: 0.6302\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 0s 493us/step - loss: 0.6428 - mean_squared_error: 0.6279 - val_loss: 0.6422 - val_mean_squared_error: 0.6305\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 0s 336us/step - loss: 0.6430 - mean_squared_error: 0.6299 - val_loss: 0.6421 - val_mean_squared_error: 0.6307\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 0s 404us/step - loss: 0.6424 - mean_squared_error: 0.6298 - val_loss: 0.6421 - val_mean_squared_error: 0.6309\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 0s 302us/step - loss: 0.6425 - mean_squared_error: 0.6290 - val_loss: 0.6421 - val_mean_squared_error: 0.6311\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.6427 - mean_squared_error: 0.6303 - val_loss: 0.6420 - val_mean_squared_error: 0.6313\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 0s 367us/step - loss: 0.6422 - mean_squared_error: 0.6302 - val_loss: 0.6420 - val_mean_squared_error: 0.6315\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 0s 211us/step - loss: 0.6424 - mean_squared_error: 0.6305 - val_loss: 0.6420 - val_mean_squared_error: 0.6317\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 0s 759us/step - loss: 0.6426 - mean_squared_error: 0.6296 - val_loss: 0.6420 - val_mean_squared_error: 0.6319\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 0s 292us/step - loss: 0.6423 - mean_squared_error: 0.6308 - val_loss: 0.6419 - val_mean_squared_error: 0.6321\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 0s 418us/step - loss: 0.6420 - mean_squared_error: 0.6308 - val_loss: 0.6419 - val_mean_squared_error: 0.6323\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.6423 - mean_squared_error: 0.6305 - val_loss: 0.6419 - val_mean_squared_error: 0.6325\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 0s 284us/step - loss: 0.6422 - mean_squared_error: 0.6314 - val_loss: 0.6418 - val_mean_squared_error: 0.6327\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 0s 298us/step - loss: 0.6430 - mean_squared_error: 0.6320 - val_loss: 0.6418 - val_mean_squared_error: 0.6329\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 0s 375us/step - loss: 0.6420 - mean_squared_error: 0.6317 - val_loss: 0.6418 - val_mean_squared_error: 0.6331\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 0s 312us/step - loss: 0.6419 - mean_squared_error: 0.6322 - val_loss: 0.6417 - val_mean_squared_error: 0.6333\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 0s 296us/step - loss: 0.6421 - mean_squared_error: 0.6314 - val_loss: 0.6417 - val_mean_squared_error: 0.6334\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 0s 236us/step - loss: 0.6427 - mean_squared_error: 0.6320 - val_loss: 0.6417 - val_mean_squared_error: 0.6336\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.6419 - mean_squared_error: 0.6329 - val_loss: 0.6417 - val_mean_squared_error: 0.6338\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 0s 372us/step - loss: 0.6418 - mean_squared_error: 0.6335 - val_loss: 0.6416 - val_mean_squared_error: 0.6340\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 0s 254us/step - loss: 0.6422 - mean_squared_error: 0.6335 - val_loss: 0.6416 - val_mean_squared_error: 0.6342\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 0s 645us/step - loss: 0.6416 - mean_squared_error: 0.6335 - val_loss: 0.6416 - val_mean_squared_error: 0.6343\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 0s 457us/step - loss: 0.6416 - mean_squared_error: 0.6340 - val_loss: 0.6416 - val_mean_squared_error: 0.6345\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 0s 623us/step - loss: 0.6417 - mean_squared_error: 0.6338 - val_loss: 0.6415 - val_mean_squared_error: 0.6347\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 0s 595us/step - loss: 0.6418 - mean_squared_error: 0.6333 - val_loss: 0.6415 - val_mean_squared_error: 0.6348\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 0s 237us/step - loss: 0.6418 - mean_squared_error: 0.6339 - val_loss: 0.6415 - val_mean_squared_error: 0.6350\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 0s 239us/step - loss: 0.6420 - mean_squared_error: 0.6342 - val_loss: 0.6415 - val_mean_squared_error: 0.6351\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 0s 830us/step - loss: 0.6419 - mean_squared_error: 0.6342 - val_loss: 0.6414 - val_mean_squared_error: 0.6353\n",
      "Epoch 227/1000\n",
      "18/18 [==============================] - 0s 423us/step - loss: 0.6416 - mean_squared_error: 0.6344 - val_loss: 0.6414 - val_mean_squared_error: 0.6355\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 0s 375us/step - loss: 0.6414 - mean_squared_error: 0.6349 - val_loss: 0.6414 - val_mean_squared_error: 0.6356\n",
      "Epoch 229/1000\n",
      "18/18 [==============================] - 0s 316us/step - loss: 0.6418 - mean_squared_error: 0.6352 - val_loss: 0.6414 - val_mean_squared_error: 0.6358\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6415 - mean_squared_error: 0.6350 - val_loss: 0.6413 - val_mean_squared_error: 0.6359\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 0s 817us/step - loss: 0.6416 - mean_squared_error: 0.6351 - val_loss: 0.6413 - val_mean_squared_error: 0.6361\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 0s 288us/step - loss: 0.6414 - mean_squared_error: 0.6351 - val_loss: 0.6413 - val_mean_squared_error: 0.6362\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 0s 340us/step - loss: 0.6416 - mean_squared_error: 0.6345 - val_loss: 0.6413 - val_mean_squared_error: 0.6363\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 0s 455us/step - loss: 0.6413 - mean_squared_error: 0.6361 - val_loss: 0.6413 - val_mean_squared_error: 0.6365\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 0s 685us/step - loss: 0.6418 - mean_squared_error: 0.6354 - val_loss: 0.6412 - val_mean_squared_error: 0.6366\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 0s 480us/step - loss: 0.6417 - mean_squared_error: 0.6351 - val_loss: 0.6412 - val_mean_squared_error: 0.6368\n",
      "Epoch 00236: early stopping\n",
      "18/18 [==============================] - 0s 186us/step\n",
      "Evaluation result on Test Data : Loss = 0.6412196755409241, Means Squared Error = 0.6367537379264832\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "train_data = X.values\n",
    "train_labels = Y.values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(8, activation='tanh'))\n",
    "# model.add(Dense(train_labels.shape[1], activation='softmax'))\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=100, mode='min', min_delta=0.01, verbose=1)\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_squared_error'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_data, train_labels, batch_size=256, epochs=1000, verbose=1,\n",
    "                    validation_data=(train_data, train_labels), callbacks=[early_stop])\n",
    "\n",
    "[test_loss, test_error] = model.evaluate(train_data, train_labels)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, Means Squared Error = {}\".format(\n",
    "    test_loss, test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def preprocess_and_predict(df):\n",
    "    dfp = preprocess(df.copy())\n",
    "    X = dfp.drop(columns=clothes).drop(columns=['Date'])\n",
    "    \n",
    "    predictions = model.predict(X.values)\n",
    "    df['New Degrees'] = [ 50 - int(np.sum(x) * 10) for x in predictions ]\n",
    "    for index, p in enumerate(predictions):\n",
    "        for index2, c in enumerate(clothes):\n",
    "            df.loc[index, c] = int(round(p[index2]))\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('data/is_it_cold.csv')\n",
    "df['Felt'] = ['normal' for _ in df['Felt']]\n",
    "now = pd.DataFrame([[time.strftime(\"%Y-%m-%d\"), '09:00', 'sunny', 4, 'normal']], columns=['Date', 'Time', 'Weather', 'Temperature', 'Felt'])\n",
    "df = df.append(now, sort=False, ignore_index=True)\n",
    "preprocess_and_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ int(np.sum(x) * 10) for x in model.predict(X.values) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
